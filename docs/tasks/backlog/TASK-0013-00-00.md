# TASK-0013-00-00: Separate Model Configuration for Different Operations

**Status**: [ ] Not Started
**Created**: 2025-01-06
**Updated**: 2025-01-06
**Assignee**: [Unassigned]
**Priority**: P2 (Medium)
**Parent Task**: N/A
**Dependencies**: None
**Estimated Effort**: S (4h)

## User Story
As a journal application administrator,
I want to configure different models for search, chat, and analysis operations,
So that I can optimize each function with the most appropriate model for performance and accuracy.

## Context & Research

### Current State Analysis
- [x] Review existing codebase in relevant directories
- [x] Document current functionality
- [x] Identify integration points
- [x] Note technical constraints

Currently, the application uses a single model configuration (`model_name` in `LLMConfig`) for all operations including semantic search, chat interactions, and batch analysis. Different operations may benefit from different models:
- **Search models**: Fast, good for semantic understanding (smaller embedding models)
- **Chat models**: Conversational, good reasoning (general-purpose models)
- **Analysis models**: Detailed analysis, structured output (larger, more capable models)

### API Documentation Review
- [x] Latest API version: Internal LLM configuration API
- [x] Relevant endpoints: `/api/config/llm`, model management endpoints
- [x] Breaking changes: Backward compatible extension of configuration
- [x] New features available: Specialized model assignment per operation type

### Technical Research
- [x] Similar implementations reviewed: OpenAI model routing, Anthropic model selection
- [x] Best practices identified: Operation-specific model optimization, fallback strategies
- [x] Performance considerations noted: Model loading overhead, memory usage
- [x] Security implications assessed: Model validation, access control

## Acceptance Criteria

### Functional Requirements
- [ ] LLMConfig model supports separate model configurations for search, chat, and analysis
- [ ] LLMService uses appropriate model for each operation type automatically
- [ ] Configuration UI allows selection of different models for different operations
- [ ] Model availability validation ensures configured models exist in Ollama
- [ ] Backward compatibility maintains existing single-model configurations
- [ ] Graceful fallback when specialized models are unavailable

### Non-Functional Requirements
- [ ] Model selection adds minimal overhead to operations
- [ ] Configuration changes take effect without requiring restart
- [ ] Model validation provides clear error messages
- [ ] Performance improvements are measurable with optimized models
- [ ] Memory usage remains efficient with multiple model configurations

## Behavioral Specifications

```gherkin
Feature: Specialized Model Configuration
  As an administrator
  I want to optimize different operations with appropriate models
  So that performance and accuracy are maximized for each use case

  Background:
    Given I have access to multiple Ollama models
    And I am on the model configuration interface

  Scenario: Configure specialized models
    Given I want to optimize different operations
    When I set the search model to "nomic-embed-text"
    And I set the chat model to "qwen3:7b"
    And I set the analysis model to "qwen3:32b"
    Then each operation should use its assigned model
    And fallback to default model should work if specialized model fails

  Scenario: Model validation
    Given I am configuring models for different operations
    When I select a model that doesn't exist in Ollama
    Then I should see a validation error
    And I should see a list of available models
    And the configuration should not be saved until valid

  Scenario: Backward compatibility
    Given I have an existing single-model configuration
    When I upgrade to the new multi-model system
    Then all operations should continue using the existing model
    And I should be able to gradually assign specialized models
    And existing configurations should not be broken

  Scenario: Performance optimization
    Given I have configured a lightweight model for search
    And a more capable model for analysis
    When I perform search operations
    Then they should complete faster than with the heavy model
    When I perform analysis operations
    Then they should provide better quality than with the light model

  Scenario Outline: Operation model assignment
    Given I have configured specialized models
    When I perform <operation_type>
    Then the system should use <expected_model>
    And the operation should complete successfully

    Examples:
      | operation_type | expected_model |
      | semantic_search | search_model |
      | chat_completion | chat_model |
      | entry_summarization | analysis_model |
      | batch_analysis | analysis_model |
```

## Implementation Plan

### Phase 1: Model Configuration Extension
1. [ ] Update LLMConfig model to include specialized model fields
2. [ ] Add model selection logic helper in LLMService
3. [ ] Implement fallback strategy for missing specialized models
4. [ ] Create database migration for new configuration fields
5. [ ] Add model validation utilities

### Phase 2: Service Layer Updates
1. [ ] Update LLMService methods to use appropriate models
2. [ ] Modify ChatService to use chat-specific model
3. [ ] Update analysis operations to use analysis model
4. [ ] Ensure semantic search uses search model
5. [ ] Add operation-specific model loading optimization

### Phase 3: Configuration Interface
1. [ ] Update configuration UI with specialized model dropdowns
2. [ ] Add model availability checking and validation
3. [ ] Create model testing functionality per operation type
4. [ ] Implement configuration export/import with new fields
5. [ ] Add model performance recommendations

### Phase 4: Testing and Validation
1. [ ] Create comprehensive test suite for model selection
2. [ ] Add performance benchmarking for different model combinations
3. [ ] Test backward compatibility with existing configurations
4. [ ] Validate error handling and fallback scenarios
5. [ ] Create documentation for optimal model selection

## Test Plan

### Unit Tests
- [ ] LLMConfig model validation and serialization
- [ ] Model selection logic with various configurations
- [ ] Fallback behavior when models are unavailable
- [ ] Configuration API endpoints with new fields

### Integration Tests
- [ ] End-to-end operation testing with specialized models
- [ ] Model switching and performance validation
- [ ] Configuration UI integration testing
- [ ] Database migration and backward compatibility

### E2E Tests
- [ ] Admin workflow: Configure and test different models
- [ ] User workflow: Verify operations use correct models
- [ ] Performance workflow: Measure optimization benefits
- [ ] Error workflow: Handle model availability issues

## Definition of Done
- [ ] All acceptance criteria met
- [ ] LLMConfig supports separate models for search, chat, and analysis
- [ ] Each operation type uses its configured model automatically
- [ ] Configuration UI functional with model validation
- [ ] Backward compatibility with existing configurations maintained
- [ ] Performance improvements measurable with optimized models
- [ ] All tests passing with >85% coverage
- [ ] Model validation prevents invalid configurations
- [ ] Documentation updated with model selection guidance
