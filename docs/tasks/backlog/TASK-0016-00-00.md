# TASK-0016-00-00: Fix Model Validation and Mocking in Tests

**Status**: [ ] Not Started
**Created**: 2025-01-09
**Updated**: 2025-01-09
**Assignee**: Claude
**Priority**: P1 (High)
**Parent Task**: TASK-0013-00-00
**Dependencies**: TASK-0013-00-00, TASK-0015-00-00
**Estimated Effort**: S (4h)

## User Story
As a developer,
I want test mocks to work properly with the new model validation logic,
So that tests can run without requiring actual Ollama connections and validate the model selection feature.

## Context & Research

### Current State Analysis
- [x] New model validation logic calls ollama.list() during initialization
- [x] Test mocks expect different ollama response structure
- [x] Model availability validation breaks existing test patterns
- [x] LLMService initialization requires proper mocking of available models

### API Documentation Review
- [x] Latest API version: Ollama Python client API
- [x] Relevant endpoints: ollama.list(), ollama.embeddings()
- [x] Breaking changes: New model validation calls during service init
- [x] New features available: Model availability checking with caching

### Technical Research
- [x] Ollama client returns {models: [{name: "model_name"}]} structure
- [x] Tests mock ollama.list differently causing 'name' key errors
- [x] Model validation happens during LLMService.__init__
- [x] Need to mock both embedding verification and model listing

## Acceptance Criteria

### Functional Requirements
- [ ] All chat API tests pass with proper model validation mocking
- [ ] LLMService initialization works in test environment
- [ ] Model selection logic properly tested with mocked available models
- [ ] Test isolation maintained with proper mock cleanup
- [ ] Model validation caching works correctly in tests

### Non-Functional Requirements
- [ ] Tests run without external Ollama dependency
- [ ] Test execution time not significantly increased
- [ ] Mock structure matches actual Ollama API responses
- [ ] Clear separation between test and production model validation

## Behavioral Specifications

```gherkin
Feature: Model Validation in Test Environment
  As a developer
  I want model validation to work properly in tests
  So that I can verify model selection logic without external dependencies

  Background:
    Given I am running tests with mocked Ollama responses
    And I have the new model validation logic enabled

  Scenario: LLMService initialization with mocked models
    Given I mock ollama.list() to return available models
    And I mock ollama.embeddings() for connection verification
    When I create an LLMService instance in a test
    Then the service should initialize successfully
    And model validation should use the mocked model list

  Scenario: Model selection with specialized models available
    Given I mock available models including specialized ones
    When I request a model for a specific operation type
    Then the service should return the appropriate specialized model
    And the selection should work without external API calls

  Scenario: Model selection with fallback scenario
    Given I mock available models excluding some specialized ones
    When I request a model for an operation with unavailable specialized model
    Then the service should fall back to the default model
    And no external API calls should be made

  Scenario: Test isolation with model caching
    Given I have multiple tests that use LLMService
    When tests run in sequence with different model configurations
    Then each test should have isolated model validation
    And cached models should not leak between tests
```

## Implementation Plan

### Phase 1: Analyze Current Test Failures
1. [ ] Review failing test logs for specific ollama mock issues
2. [ ] Identify all places where ollama.list() is called
3. [ ] Document current mock structures vs required structures
4. [ ] Map out test dependencies on LLMService initialization

### Phase 2: Fix Ollama Mocking
1. [ ] Update ollama.list() mocks to return correct structure
2. [ ] Ensure ollama.embeddings() mocks work for connection verification
3. [ ] Create reusable mock fixtures for common test scenarios
4. [ ] Add proper mock cleanup between tests

### Phase 3: Update Test Infrastructure
1. [ ] Modify test setup to handle model validation
2. [ ] Add test-specific configuration for model selection
3. [ ] Ensure test isolation with proper service initialization
4. [ ] Create helper functions for common mocking patterns

### Phase 4: Validation and Testing
1. [ ] Run all affected tests to verify fixes
2. [ ] Test different model configuration scenarios
3. [ ] Verify test isolation and cleanup
4. [ ] Performance test with updated mocking

## Test Plan

### Unit Tests
- [ ] LLMService initialization with various mock configurations
- [ ] Model validation logic with different available model sets
- [ ] Model selection fallback scenarios
- [ ] Cache behavior in test environment

### Integration Tests
- [ ] Chat API tests with mocked model validation
- [ ] Configuration storage tests with new schema
- [ ] End-to-end workflows with model selection
- [ ] Error handling with invalid model configurations

### Mock Validation Tests
- [ ] Verify mock structure matches real Ollama responses
- [ ] Test mock cleanup between test cases
- [ ] Validate test isolation
- [ ] Performance impact of updated mocking

## Definition of Done
- [ ] All chat API tests pass with updated mocks
- [ ] LLMService initialization works in all test scenarios
- [ ] Model validation properly mocked and tested
- [ ] Test execution time acceptable
- [ ] No external dependencies required for tests
- [ ] Proper test isolation maintained
- [ ] Mock structure accurately represents Ollama API
